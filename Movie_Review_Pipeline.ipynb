{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4N0OkqVmfRq"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import nltk\n",
        "from nltk.corpus import movie_reviews\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('movie_reviews')\n",
        "\n",
        "# Build a list of dictionaries\n",
        "data = [\n",
        "    {'review': movie_reviews.raw(f), 'label': category}\n",
        "    for category in movie_reviews.categories()\n",
        "    for f in movie_reviews.fileids(category)\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Shuffle rows\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "Y8pGEjHSmmTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "rWSCOJnKmz7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['review'], df['label'], test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "06tc57oZmr6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')  # for WordNet data\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "8kIeue5LnS77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "owat15UGqX_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_analyzer(text):\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    # Remove non-alphabetic characters\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    # Split words\n",
        "    words = text.split()\n",
        "    # Lemmatize each word\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    return [lemmatizer.lemmatize(word) for word in words]"
      ],
      "metadata": {
        "id": "GdyC3x9pnVz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(\n",
        "    analyzer=lemmatize_analyzer,\n",
        "    stop_words='english',\n",
        "    max_features=5000\n",
        ")\n"
      ],
      "metadata": {
        "id": "YmMBGJsUn6_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline([\n",
        "    ('vectorizer', vectorizer),\n",
        "    ('classifier', RandomForestClassifier(n_estimators=200, random_state=41))\n",
        "])\n"
      ],
      "metadata": {
        "id": "4Z6p26YSoDv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = pipeline.named_steps['vectorizer']\n",
        "classifier = pipeline.named_steps['classifier']"
      ],
      "metadata": {
        "id": "b3_i03f1oHMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "W9-Iu5Qcpy-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = vectorizer.get_feature_names_out()\n",
        "importances = classifier.feature_importances_\n",
        "\n",
        "df_features = pd.DataFrame({\n",
        "    'word': feature_names,\n",
        "    'importance': importances\n",
        "})"
      ],
      "metadata": {
        "id": "uGBT9MBKpeZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_features"
      ],
      "metadata": {
        "id": "05gJFn9npjoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_vec = vectorizer.transform(X_train)\n",
        "X_dense = X_train_vec.toarray()\n",
        "\n",
        "# Binary labels for convenience\n",
        "y_train_bin = (y_train == 'pos').astype(int)\n",
        "\n",
        "# Count per class\n",
        "count_positive = X_dense[y_train_bin==1].sum(axis=0)\n",
        "count_negative = X_dense[y_train_bin==0].sum(axis=0)\n",
        "\n",
        "# Total count across all documents\n",
        "total_count = X_dense.sum(axis=0)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "importances = classifier.feature_importances_\n",
        "\n",
        "df_features = pd.DataFrame({\n",
        "    'word': feature_names,\n",
        "    'importance': importances,\n",
        "    'count_positive': count_positive,\n",
        "    'count_negative': count_negative,\n",
        "    'total_count': total_count\n",
        "})\n",
        "\n",
        "# Sort by importance\n",
        "df_features_sorted = df_features.sort_values(by='importance', ascending=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "qhYs8Fpwqeji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_features_sorted.head(20)"
      ],
      "metadata": {
        "id": "udZn56xzrDv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_features_sorted.tail(20)"
      ],
      "metadata": {
        "id": "gurMMfdYr6Rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SDbsFsN0r9vB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}